{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jhonr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Librerías para manipulación de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Librerías para procesamiento de texto\n",
    "import re\n",
    "import string\n",
    "import langdetect\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Extracción de palabras clave\n",
    "import yake\n",
    "from rake_nltk import Rake\n",
    "\n",
    "# Similitud semántica\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Visualización y análisis\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Modelado y evaluación\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Utilidades\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vista preliminar del archivo:\n",
      "                                             Authors      Area  \\\n",
      "0  Guevara-Cuellar C.A.; Parody-Rúa E.; Garcia-Pe...  Medicina   \n",
      "1  Fernández-Niño J.A.; Bonilla-Tinoco L.J.; Manr...  Medicina   \n",
      "2      Álvarez A.; Jiménez Á.; Méndez J.; Murillo E.  Medicina   \n",
      "3           Giraldo J.; Acosta C.; Giraldo-Grueso M.  Medicina   \n",
      "4  Cardozo D.F.R.; Martinez W.J.A.; Lopez J.F.C.;...  Medicina   \n",
      "\n",
      "                                   Author full names  \\\n",
      "0  Guevara-Cuellar, César Augusto (23489087300); ...   \n",
      "1  Fernández-Niño, Julián Alfredo (20433473700); ...   \n",
      "2  Álvarez, Andree (59239404100); Jiménez, Ángel ...   \n",
      "3  Giraldo, Juan (7003797713); Acosta, Claudia (5...   \n",
      "4  Cardozo, Diego F Rincon (57194755896); Martine...   \n",
      "\n",
      "                                               Title  Year  \\\n",
      "0  Cost Effectiveness of Combination Therapy Vers...  2018   \n",
      "1  Work status, retirement, and depression in old...  2018   \n",
      "2  Chemical and biological study of Eugenia Stipi...  2018   \n",
      "3  Frequency of anesthetic overdose with mean alv...  2018   \n",
      "4  Description and Clinical Assessment of the Bon...  2018   \n",
      "\n",
      "                                        Source title  Cited by  \\\n",
      "0                    Value in Health Regional Issues         3   \n",
      "1                            SSM - Population Health        43   \n",
      "2  Asian Journal of Pharmaceutical and Clinical R...         5   \n",
      "3                               Medical Gas Research         5   \n",
      "4   The journal of hand surgery Asian-Pacific volume         0   \n",
      "\n",
      "                                DOI  \\\n",
      "0        10.1016/j.vhri.2018.09.004   \n",
      "1       10.1016/j.ssmph.2018.07.008   \n",
      "2  10.22159/ajpcr.2018.v11i12.27253   \n",
      "3          10.4103/2045-9912.248265   \n",
      "4         10.1142/S2424835518500510   \n",
      "\n",
      "                                        Affiliations  \\\n",
      "0  Facultad de Ciencias de la Salud, Centro de Es...   \n",
      "1  Public Health Department, Universidad del Nort...   \n",
      "2  Department Chemistry, Natural Products Researc...   \n",
      "3  Cardiothoracic Anesthesia Department, Fundació...   \n",
      "4  Hospital Internacional de Colombia, Bucaramang...   \n",
      "\n",
      "                                            Abstract  \\\n",
      "0  Objectives: To estimate the incremental cost e...   \n",
      "1  The aim of the present study was to analyse th...   \n",
      "2  Objective: The main objective of this study is...   \n",
      "3  This study reported the frequency of anestheti...   \n",
      "4  BACKGROUND: Pellegrini's surgical technique is...   \n",
      "\n",
      "                                     Author Keywords  \\\n",
      "0  5-α-reductase inhibitors; benign prostatic hyp...   \n",
      "1  Depression; Older adults; Retirement; Social d...   \n",
      "2  Anthelmintic activity; Antioxidant; Arazá; Gas...   \n",
      "3  general anesthesia; drug overdose; biespectral...   \n",
      "4  Bone block; Ligament reconstruction; Osteoarth...   \n",
      "\n",
      "                                      Index Keywords  \n",
      "0  5-alpha Reductase Inhibitors; Adrenergic alpha...  \n",
      "1  adult; age distribution; Article; China; cross...  \n",
      "2                                                NaN  \n",
      "3  anesthetic agent; sevoflurane; adult; age; alt...  \n",
      "4  Bone Screws; Carpometacarpal Joints; Female; H...  \n",
      "\n",
      "Columnas encontradas en el archivo:\n",
      "['Authors', 'Area', 'Author full names', 'Title', 'Year', 'Source title', 'Cited by', 'DOI', 'Affiliations', 'Abstract', 'Author Keywords', 'Index Keywords']\n",
      "\n",
      "Conteo de valores nulos por columna:\n",
      "Authors                0\n",
      "Area                   0\n",
      "Author full names      0\n",
      "Title                  0\n",
      "Year                   0\n",
      "Source title           0\n",
      "Cited by               0\n",
      "DOI                  184\n",
      "Affiliations           0\n",
      "Abstract               0\n",
      "Author Keywords      482\n",
      "Index Keywords       700\n",
      "dtype: int64\n",
      "\n",
      "Total de resúmenes válidos encontrados: 2971\n"
     ]
    }
   ],
   "source": [
    "# Ruta del archivo de datos\n",
    "ruta_archivo = r\"C:\\Users\\Jhonr\\Project_ODS_BERT\\Data_total\\Dataset_Inferencia.xlsx\"\n",
    "\n",
    "# Carga del archivo Excel\n",
    "df = pd.read_excel(ruta_archivo)\n",
    "\n",
    "# Visualización inicial de las primeras filas\n",
    "print(\"Vista preliminar del archivo:\")\n",
    "print(df.head())\n",
    "\n",
    "# Verificación de columnas disponibles\n",
    "print(\"\\nColumnas encontradas en el archivo:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Verificación de valores nulos por columna\n",
    "print(\"\\nConteo de valores nulos por columna:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Filtrado de registros con resumen válido\n",
    "df = df[df['Abstract'].notnull() & (df['Abstract'].str.strip() != '')]\n",
    "\n",
    "# Revisión de registros después del filtrado\n",
    "print(f\"\\nTotal de resúmenes válidos encontrados: {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jhonr\\AppData\\Local\\Temp\\ipykernel_24476\\4261797259.py:9: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  texto = BeautifulSoup(texto, \"html.parser\").get_text()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ejemplo de resumen limpio para YAKE:\n",
      "objectives: to estimate the incremental cost effectiveness ratio of pharmacological treatment for benign prostatic hyperplasia from the payer's perspective. methods: the cost effectiveness of 5 mg finasteride, 0.5 mg dutasteride, 10 mg alfuzosin, 10 mg terazosin, 0.4 mg tamsulosin, 4 mg doxazosin, and the combination therapy of 5 mg finasteride and 8 mg doxazosin was evaluated using a markov model over a 30-year period. the costs were estimated using national tariffs and were reported in us dollars. cost and effectiveness outcomes were discounted at a rate of 5% per year. men (aged ≥40 years) with moderate to severe lower urinary tract symptoms and uncomplicated benign prostatic hyperplasia were included in the analysis. outcomes included costs and quality-adjusted life-years. a probabilistic sensitivity analysis was performed on important parameters with monte-carlo simulation. results: finasteride alone or in combination with doxazosin dominated all α-blockers. after excluding dominated alternatives, the incremental cost–utility ratio for combination therapy was $377 per quality-adjusted life-year, being a cost-effective alternative using the threshold of $15 000. model results were robust to changes in costs, utility weights, and probabilities. acceptability curves consistently demonstrated that the combination therapy was most likely cost-effective. conclusions: the combination of finasteride and doxazosin is cost-effective compared with dutasteride, tamsulosin, terazosin, and alfuzosin in patients with benign prostatic hyperplasia with moderate or severe symptoms who are older than 40 years. © 2018\n",
      "\n",
      "Ejemplo de resumen limpio para BERT:\n",
      "Objectives To estimate the incremental cost effectiveness ratio of pharmacological treatment for benign prostatic hyperplasia from the payers perspective Methods The cost effectiveness of 5 mg finasteride 05 mg dutasteride 10 mg alfuzosin 10 mg terazosin 04 mg tamsulosin 4 mg doxazosin and the combination therapy of 5 mg finasteride and 8 mg doxazosin was evaluated using a Markov model over a 30year period The costs were estimated using national tariffs and were reported in US dollars Cost and effectiveness outcomes were discounted at a rate of 5 per year Men aged 40 years with moderate to severe lower urinary tract symptoms and uncomplicated benign prostatic hyperplasia were included in the analysis Outcomes included costs and qualityadjusted lifeyears A probabilistic sensitivity analysis was performed on important parameters with MonteCarlo simulation Results Finasteride alone or in combination with doxazosin dominated all αblockers After excluding dominated alternatives the incremental costutility ratio for combination therapy was 377 per qualityadjusted lifeyear being a costeffective alternative using the threshold of 15 000 Model results were robust to changes in costs utility weights and probabilities Acceptability curves consistently demonstrated that the combination therapy was most likely costeffective Conclusions The combination of finasteride and doxazosin is costeffective compared with dutasteride tamsulosin terazosin and alfuzosin in patients with benign prostatic hyperplasia with moderate or severe symptoms who are older than 40 years  2018\n"
     ]
    }
   ],
   "source": [
    "# Limpieza específica para YAKE (basada en Yake_mejor_test_6-7-13.ipynb)\n",
    "def limpiar_texto_yake(texto):\n",
    "    texto = str(texto).strip()\n",
    "    texto = texto.lower()\n",
    "    return texto\n",
    "\n",
    "# Limpieza específica para BERT (basada en GPU_Area_Clases6_7_9_13_OTROS_GPT.ipynb)\n",
    "def limpiar_texto_bert(texto):\n",
    "    texto = BeautifulSoup(texto, \"html.parser\").get_text()\n",
    "    texto = re.sub(r'\\n+', ' ', texto)\n",
    "    texto = re.sub(r'\\s+', ' ', texto)\n",
    "    texto = re.sub(r'[^\\w\\s]', '', texto)\n",
    "    return texto.strip()\n",
    "\n",
    "# Aplicar a columnas\n",
    "df[\"abstract_yake\"] = df[\"Abstract\"].apply(limpiar_texto_yake)\n",
    "df[\"abstract_bert\"] = df[\"Abstract\"].apply(limpiar_texto_bert)\n",
    "\n",
    "# Mostrar ejemplo\n",
    "print(\"\\nEjemplo de resumen limpio para YAKE:\")\n",
    "print(df[\"abstract_yake\"].iloc[0])\n",
    "\n",
    "print(\"\\nEjemplo de resumen limpio para BERT:\")\n",
    "print(df[\"abstract_bert\"].iloc[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FASE 1 – YAKE (ODS 6, 7 y 13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paso 1: Definir diccionario de keywords por ODS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionario de palabras clave representativas por ODS\n",
    "keywords_ods = {\n",
    "    \"ODS 6\": [\"water\", \"sanitation\", \"wastewater\", \"hygiene\", \"clean\", \"access\", \"drinking\", \"sewage\", \"treatment\", \"infrastructure\"],\n",
    "    \"ODS 7\": [\"energy\", \"renewable\", \"solar\", \"electricity\", \"clean\", \"affordable\", \"efficiency\", \"power\", \"grid\", \"supply\"],\n",
    "    \"ODS 13\": [\"climate\", \"emissions\", \"carbon\", \"resilience\", \"adaptation\", \"warming\", \"sustainability\", \"mitigation\", \"ghg\", \"environment\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paso 2: Inicializar YAKE y función para obtener keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_extractor = yake.KeywordExtractor(lan=\"en\", n=1, top=10)\n",
    "\n",
    "def obtener_keywords_yake(texto):\n",
    "    try:\n",
    "        keywords = kw_extractor.extract_keywords(texto)\n",
    "        return [kw for kw, score in keywords]\n",
    "    except:\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paso 3: Calcular intersección con listas ODS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asignar_ods_por_yake(keywords_extraidas):\n",
    "    max_match = 0\n",
    "    mejor_ods = None\n",
    "    for ods, lista_ods in keywords_ods.items():\n",
    "        coincidencias = len(set(keywords_extraidas).intersection(set(lista_ods)))\n",
    "        if coincidencias > max_match:\n",
    "            max_match = coincidencias\n",
    "            mejor_ods = ods\n",
    "    return mejor_ods if max_match >= 1 else None  # Umbral mínimo de coincidencia = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paso 4: Aplicar YAKE a todos los resúmenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribución de predicciones YAKE:\n",
      "prediccion_yake\n",
      "None      2632\n",
      "ODS 7      151\n",
      "ODS 6      141\n",
      "ODS 13      47\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Extraer keywords y predecir ODS\n",
    "df[\"keywords_yake\"] = df[\"abstract_yake\"].apply(obtener_keywords_yake)\n",
    "df[\"prediccion_yake\"] = df[\"keywords_yake\"].apply(asignar_ods_por_yake)\n",
    "\n",
    "# Revisión rápida\n",
    "print(\"\\nDistribución de predicciones YAKE:\")\n",
    "print(df[\"prediccion_yake\"].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Código base para la inferencia con BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1. Filtro y separación por área"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar resúmenes no clasificados por YAKE\n",
    "df_bert = df[df[\"prediccion_yake\"].isna()].copy()\n",
    "\n",
    "# Dividir por área\n",
    "df_med = df_bert[df_bert[\"Area\"] == \"Medicina\"].copy()\n",
    "df_ing = df_bert[df_bert[\"Area\"] == \"Ingeniería\"].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Preparar clase para inferencia con HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Clase personalizada de Dataset para HuggingFace\n",
    "class ODSDataset(Dataset):\n",
    "    def __init__(self, textos, tokenizer, max_length=256):\n",
    "        self.encodings = tokenizer(textos, truncation=True, padding=\"max_length\", max_length=max_length)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 3. Función para cargar modelo y predecir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inferir_con_bert(df_area, ruta_modelo):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(ruta_modelo)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(ruta_modelo)\n",
    "\n",
    "    textos = df_area[\"abstract_bert\"].tolist()\n",
    "    dataset = ODSDataset(textos, tokenizer)\n",
    "\n",
    "    trainer = Trainer(model=model)\n",
    "    predicciones = trainer.predict(dataset)\n",
    "    \n",
    "    labels_pred = torch.argmax(torch.tensor(predicciones.predictions), axis=1).tolist()\n",
    "\n",
    "    # Mapear ID de clase a nombre\n",
    "    id2label = {0: \"ODS 3\", 1: \"ODS 9\", 2: \"OTROS\"}\n",
    "    etiquetas = [id2label[i] for i in labels_pred]\n",
    "\n",
    "    df_area[\"prediccion_bert\"] = etiquetas\n",
    "    return df_area\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Ejecutar inferencia por área"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Incorrect path_or_model_id: 'C:\\Users\\Jhonr\\Project_ODS_BERT\\Modelos\\Medicina'. Please provide either the path to a local folder or the repo_id of a model on the Hub.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Jhonr\\anaconda3\\envs\\ods_unificado\\lib\\site-packages\\transformers\\utils\\hub.py:402\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    401\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 402\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Jhonr\\anaconda3\\envs\\ods_unificado\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m--> 106\u001b[0m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Jhonr\\anaconda3\\envs\\ods_unificado\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:160\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[1;34m(repo_id)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n\u001b[1;32m--> 160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must use alphanumeric chars or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m are\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m forbidden, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m cannot start or end the name, max length is 96:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    163\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    164\u001b[0m     )\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id:\n",
      "\u001b[1;31mHFValidationError\u001b[0m: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: 'C:\\Users\\Jhonr\\Project_ODS_BERT\\Modelos\\Medicina'.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m ruta_modelo_ing \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mJhonr\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mProject_ODS_BERT\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mModelos\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mIngenieria\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Aplicar a cada subconjunto\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m df_med_resultado \u001b[38;5;241m=\u001b[39m \u001b[43minferir_con_bert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_med\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mruta_modelo_med\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m df_ing_resultado \u001b[38;5;241m=\u001b[39m inferir_con_bert(df_ing, ruta_modelo_ing)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Unir resultados\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m, in \u001b[0;36minferir_con_bert\u001b[1;34m(df_area, ruta_modelo)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minferir_con_bert\u001b[39m(df_area, ruta_modelo):\n\u001b[1;32m----> 2\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mruta_modelo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     model \u001b[38;5;241m=\u001b[39m AutoModelForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(ruta_modelo)\n\u001b[0;32m      5\u001b[0m     textos \u001b[38;5;241m=\u001b[39m df_area[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabstract_bert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[1;32mc:\\Users\\Jhonr\\anaconda3\\envs\\ods_unificado\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:834\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    831\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    833\u001b[0m \u001b[38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[1;32m--> 834\u001b[0m tokenizer_config \u001b[38;5;241m=\u001b[39m \u001b[43mget_tokenizer_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tokenizer_config:\n\u001b[0;32m    836\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenizer_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Jhonr\\anaconda3\\envs\\ods_unificado\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:666\u001b[0m, in \u001b[0;36mget_tokenizer_config\u001b[1;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[0;32m    663\u001b[0m     token \u001b[38;5;241m=\u001b[39m use_auth_token\n\u001b[0;32m    665\u001b[0m commit_hash \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 666\u001b[0m resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    683\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Jhonr\\anaconda3\\envs\\ods_unificado\\lib\\site-packages\\transformers\\utils\\hub.py:466\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere was a specific connection error when trying to load \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HFValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 466\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    467\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect path_or_model_id: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Please provide either the path to a local folder or the repo_id of a model on the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    468\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n",
      "\u001b[1;31mOSError\u001b[0m: Incorrect path_or_model_id: 'C:\\Users\\Jhonr\\Project_ODS_BERT\\Modelos\\Medicina'. Please provide either the path to a local folder or the repo_id of a model on the Hub."
     ]
    }
   ],
   "source": [
    "# Rutas a tus modelos entrenados\n",
    "ruta_modelo_med = r\"C:\\Users\\Jhonr\\Project_ODS_BERT\\Modelos\\Medicina\"\n",
    "ruta_modelo_ing = r\"C:\\Users\\Jhonr\\Project_ODS_BERT\\Modelos\\Ingenieria\"\n",
    "\n",
    "# Aplicar a cada subconjunto\n",
    "df_med_resultado = inferir_con_bert(df_med, ruta_modelo_med)\n",
    "df_ing_resultado = inferir_con_bert(df_ing, ruta_modelo_ing)\n",
    "\n",
    "# Unir resultados\n",
    "df_bert_final = pd.concat([df_med_resultado, df_ing_resultado])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ods_unificado",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
